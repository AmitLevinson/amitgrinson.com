---
title: Israeli elections on Twitter
author: Amit Levinson
date: '2020-04-20'
slug: israeli-elections-on-twitter
categories: [R]
tags: [rtweet, tidytext]
subtitle: ''
summary: 'Analyzing tweets from the the Israeli elections week'
authors: []
featured: no
draft: false
image:
  caption: ''
  focal_point: ''
  preview_only: no
projects: []
editor_options: 
  chunk_output_type: console
layout: single
---

### Introduction

Israel had its 3rd election within 12 months on March 2, 2020. This is because our Knesset - Hebrew term for house of representatives - wasnâ€™t able to form or hold a government after each of the previous elections. As I wonâ€™t get into the politics of why they didnâ€™t succeed in forming one (get it? politics :wink:), I do want to take the opportunity and analyze some tweets posted in the time before and after the elections.  
When we think of a data aggregating tweets, many questions arise - who, what, when, where and more about our data. Namely, with the collected data I want to answer the following questions:

1.  What was the frequency of tweets associated with the word â€˜electionsâ€™?
2.  Who tweeted the most?
3.  What was the most common \#Hashtag tweeted?
4.  Which tweet was most liked and which was retweeted the most?
5.  What were the most common words and bigrams (two words) in tweets?

### Gathering the data <i class="fab fa-twitter"></i>

Twitterâ€™s API allows scraping **6-9 days back for free**. Therefore, I scraped the data already on March 7, 2020 and saved it for later use.

Letâ€™s start with the packages weâ€™ll use:

``` r
library(rtweet)
library(tidyverse)
library(tidytext)
library(igraph)
library(hrbrthemes)
library(ggraph)
library(extrafont)
```

I could use a consistent plot theme throughout the post but Iâ€™ll probably be editing each one a bit, while also some are not our regular graphs. With that said, There are some tweaks that will be consistent acorss several of the plots. Therefore, letâ€™s create a theme function as a supplement to all other theme arguments Iâ€™ll use that will save a few lines of code:

``` r
mini_theme <- function(family = "Roboto Condensed", tsize = 16) {
  theme_classic() +
  theme(text = element_text(family = family),
        axis.ticks = element_blank(),
        axis.line = element_blank(),
        plot.title = element_text(size = tsize))}
```

Next weâ€™ll gather the tweets we need:

``` r
elections_raw <- search_tweets("×‘×—×™×¨×•×ª", n = 18000, retryonratelimit = TRUE)
```

To gather the tweets we can use the [{rtweet}](https://rtweet.info/) package which is amazing for collecting Twitter data. As I mentioned earlier, I already scraped the data a few days after the elections but left the command here to show what we did and how easy it is to do it. I searched only one term, â€˜electionsâ€™ in Hebrew, and rtweet gathered all tweets containing that word.

What did our search yield? Letâ€™s have a look:

``` r
dim(elections)
```

    ## [1] 16560    90

16,560 rows and 90 columns! As we can see, the `{rtweet}` package brings back a lot of information!

#### Some Caveats:

Before we begin, I will say this post doesnâ€™t aim to be representative of the discussions that were held during the election period. As a matter of fact, nor does it aim to be representative of the twitter discussions surrounding the elections. this is due to two main reasons:

1.  Twitter isnâ€™t common in Israel at all. Iâ€™m not sure whatâ€™s the usage rate but itâ€™s definitely not representative of the Israeli population.

2.  I searched for only one word - elections (in Hebrew) - which yielded some 16560 tweets. This is definitely not a large enough pool of tweets to claim for representation.

With that said, the data gathered provides an opportunity to look at some Twitter data from the election period and motivate others to use the `{rtweet}` package, so why not give it a go.

### Tweet frequency

First, letâ€™s see how the tweets distribute across the time span we searched for. we can create a quick time plot using the `ts_plot()` argument from the `{rtweet}` package:

``` r
elections %>% 
  ts_plot("2 hours")+
  geom_line(size = 1, color = "black")+
  mini_theme()+
  scale_x_datetime(date_breaks = "1 day",date_labels = "%d %b")+
  labs(x= NULL, y = NULL,
       title = "Tweet frequency throughout the Israeli elections week",
       subtitle = "Tweets aggregated by two-hour interval. Only tweets containing the word 'elections'\nin Hebrew were gathered")+
  geom_text(aes(x = as.POSIXct("2020-03-02 23:00:00"), y = 435, label = "10 PM:\nPolls close"),
            hjust = 0, size = 3, family = "Roboto Condensed")+
  geom_vline(xintercept = as.POSIXct("2020-03-02 22:00"),linetype = "dashed", size = 0.5, color = "black", alpha = 5/10)+
  theme(plot.subtitle = element_text(color = "gray70"))
```

<img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-6-1.png" width="672" style="display: block; margin: auto;" />

Interesting - we see the number of tweets during the closing time is equivalent to that of midday on March 4th. Most of the votes were counted by the end of March 3rd, so I canâ€™t really put my finger on what this jump represents. After all, I collected tweets containing our word so it could have been that many people tweeted that specific term in that time slot. Anyway, I wasnâ€™t able to find anything interesting that happened on the news that day but feel free to explore and offer suggestions.

### Users with most tweets

Next, letâ€™s look at who tweeted the most:

``` r
elections %>% 
  count(screen_name, sort = T) %>% 
  slice(1:15) %>% 
  mutate(screen_name = reorder(screen_name,n)) %>% 
  ggplot(aes(x= screen_name, y= n))+
  geom_col(fill = "gray70")+
  coord_flip()+
  scale_y_continuous(breaks = seq(0,180, 30), labels = seq(0,180,30))+
  labs(x = "Screen name", y = "Number of tweets", title = "Top 15 users tweeting the word 'elections'")+
  mini_theme()+
  theme(text = element_text(family = "Calibri"),
        axis.text = element_text(size = 12),
        axis.title.y = element_blank())
```

<img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-7-1.png" width="768" style="display: block; margin: auto;" />

We see that many news companies tweeted a lot using the word â€˜electionsâ€™: â€˜newisrael13â€™, â€˜kann_newsâ€™, â€˜MaarivOnlineâ€™, â€˜RotterNewsâ€™, â€˜bahazit_newsâ€™, â€˜RotterNetâ€™. I personnaly donâ€™t recognize the rest, but on the other hand I use Twitter mostly to follow `R` and academic related tweets, not necessarily Israeli politics.

### Common Hashtags

When using the `{rtweet}` package to gather twitter data, one of the variables collected is the hashtags used in tweets. Although it doesnâ€™t require too many lines of code to extract hashtags out of text, I think this is an amazing feature that shows the effort and details [Michael W. Kearney](https://mikewk.com/) and contributors put into the package.

According to [Wikipedia](https://en.wikipedia.org/wiki/Hashtag), a â€˜Hashtagâ€™ â€œis a type of metadata tag used on social networks such as Twitter and other microblogging services.â€, that basically tags the message with a specific theme. This helps to see trends and themes in a macro level.

OK then, letâ€™s see what we have:

``` r
hashtags <- elections %>% 
  select(hashtags) %>% 
  unlist() %>% 
  as.tibble() %>% 
  mutate(value = tolower(value)) %>% 
  count(value, name = "Count", sort = T) %>%
  mutate(value = reorder(value, Count),
         iscorona = ifelse(value == "×§×•×¨×•× ×”" | value == "coronavirus", "y", "n")) %>% 
  filter(!is.na(value)) %>% 
  slice(1:20)

ggplot(data = hashtags, aes(x = Count, y = value, fill = iscorona))+
  geom_col(show.legend = FALSE)+
  scale_fill_manual(values = c(y = "#1DA1F2", n = "gray70"))+
  labs(y = NULL, x = "Number of Tweets", title = "Top 20 Hashtags addressing the Israeli elections")+
  mini_theme()+
  theme(text = element_text(family = "Calibri"),
        axis.text = element_text(size = 12))
```

<img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-9-1.png" width="672" style="display: block; margin: auto;" />

The tweets include pretty much what we expect - hashtags about the elections - with the two leading ones being â€˜electionsâ€™ and â€˜elections2020â€™. We also see a peculiar hashtag â€˜right_following_right_peopleâ€™, and others such as â€˜Netanyahuâ€™ (the Prime minister at the time), â€˜Israelâ€™ and others.  
I highlighted in blue an interesting hashtag at the time - <font color="#1DA1F2">**Corona**</font> (in hebrew) and <font color="#1DA1F2">**coronavirus**</font>. The elections were held on March 2, 2020, a little bit after the first cases reached Israel. Little did we know how it will affect us (Iâ€™m finalzing this post on April 18, 2020, and only now weâ€™re starting to get back to routine. Slowly)

### Most liked and retweeted

Letâ€™s have a look at which tweet was **most liked**. Twitter doesnâ€™t define it as â€˜likesâ€™ but as â€˜favoriteâ€™, or at least in the data that is collected through the `{rtweet}` package. Since I will want to gather the most of something - both favorite and later retweeted - Iâ€™ll create a function that will minimize re-writing the code.  
<br>
The function takes in a variable, reorders our dataset according to the variable we declared, extracts the first row and then pulls (extracts) the status id of that tweet. Lastly, the `blogdown::shortcode` enables to embed tweets, youtube videos and more into a blogdown post such as this, so we end the function by inserting our status id into that. For those just getting into functions notice that within the `arrange` argument we insert our variable in two curly brackets {{}}. This is a powerful feature of `{rlang}` when you want to manipulate a variable in a dataframe within a function. You can read more about that [here](https://www.tidyverse.org/blog/2019/06/rlang-0-4-0/).

``` r
get_most <- function(var){
elections %>% 
  arrange(desc({{var}})) %>% 
  .[1,] %>% 
  pull(status_id) %>% 
  blogdown::shortcode('tweet',.)
}
```

Now Letâ€™s see which tweet was **most liked** during that week:

<center>
{{% tweet "1234584864415997952" %}}
</center>

The tweet is by â€˜Amit Segalâ€™ - an Israeli news reporter - and it says (my translation):

> â€œMore than anything, Iâ€™m glad there wonâ€™t be another elections for my family that suffered in honors a year and a quarter. Reut, Ivri and Aner :heart_eyes:â€

Ha, interestingly he wrote it before the end of the elections, hopefully heâ€™s right!

Now letâ€™s look at the **most re-tweeted** tweet:

<center>
{{% tweet "1233342393740603394" %}}
</center>

The tweet is by Benjamin Netanyahu, at the time the prime minister of Israel, who writes:

> â€œIf the recording of Gantzâ€™s advisor is orcherstrated and fabricated (according to Gantzâ€™s words just now), why did Gantz fire him? Gantzâ€™s advisor was fired because he said the truth everyone knows: Gantz canâ€™t be a prime minister. We can. 2 more mandates to the Likkud and we are taking the country out of the plonter, preventing another election and form a governmentâ€

This came after the exposure of a secret recording of Gantz in a closed meeting, A few days before election day.

## Wordcloud and bigrams

Letâ€™s have a look at two more text-related analyses:

1.  A word-cloud  
2.  Bigrams (two-words) from our text

We could try out more algorthims but Iâ€™ll save them for a different post (feel free to try on your own).

### Wordcloud

In order to tackle the wordcloud, Iâ€™ll break up all the tweets into **single words**, filter any Hebrew stop words (file found online) and all English words. The decision to filter English words is mainly because Iâ€™m interested in the Hebrew sentences, but also because most the common English words used in our data are those of Twitter user names cited when replying to a tweet:

``` r
he_stopwords <- read_tsv("https://raw.githubusercontent.com/gidim/HebrewStopWords/master/heb_stopwords.txt", col_names = "word")

election_token <- elections %>% 
  unnest_tokens(word, text) %>% 
  select(word) %>%
  anti_join(he_stopwords) %>% 
  count(word, sort = T) %>%
  filter(!grepl("([a-z]+|×‘×—×™×¨×•×ª)", word), n>= 150)
```

Now we can create a wordcloud of words appearing more than 150 times using `{wordcloud2}` package[^1]:

``` r
wordcloud2::wordcloud2(election_token, color = "#1DA1F2", shape = "circle")
```

<div class="figure" style="text-align: center">

<img src="wc.png" alt="Wordcloud excludes Hebrew stop words and the word 'elections'" width="550" />
<p class="caption">
Figure 1: Wordcloud excludes Hebrew stop words and the word â€˜electionsâ€™
</p>

</div>

<br>

What we can see is many of the words weâ€™d expect: Political candidates, government, fourth (in the context of fourth elections), partisâ€™ names and more. Iâ€™ll provide a more thorough discussion following our bigram plot below, as I believe it addresses many of the same words.

### Common Bigrams

Like we did before, we can break up our text data into **two word** observations, also known as bigrams. In order to account for all combinations, we break up the sentence to fit all possible options. For example, assume we have the following sentence:

â€œDanny went to vote yesterdayâ€

Using the `unnest_tokens` weâ€™ll break the sentence into the following bigrams:

1.  Danny went  
2.  went to  
3.  to vote  
4.  vote yesterday

Which gives us all possible options. We will also include two columns consisting of the bigram broken up into single words. This will help in filtering out bigrams containing Hebrew stop words or English words. Iâ€™ll not run through the following code but instead will point you to [David Ronbinson](http://varianceexplained.org/) & [Julia Silge](https://juliasilge.com/) [â€˜Text Mining with Râ€™ Book](https://www.tidytextmining.com/) for further reading.

``` r
elec_bigram <- elections %>%
  select(text) %>% 
  unnest_tokens(bigram, text, token = "ngrams", n = 2) %>%
  separate(bigram, into = c("word1", "word2"), sep = " ", remove = FALSE) %>% 
  filter(!word1 %in% he_stopwords$word,
         !word2 %in% he_stopwords$word,
         !grepl("([a-z]+|×‘×—×™×¨×•×ª)", bigram)) %>% 
  count(word1, word2, sort = T) %>% 
  slice(1:45) %>%
  graph_from_data_frame()

p_arrow <- arrow(type = "closed", length = unit(.1, "inches"))

ggraph(elec_bigram, layout = "fr")+
  geom_edge_link(aes(edge_alpha = n), arrow = p_arrow, end_cap = circle(.04, "inches"), show.legend = FALSE)+
  geom_node_point(color = "lightblue", size = 3)+
  geom_node_text(aes(label = name), vjust = 1, hjust = 1, family = "Calibri")+
  theme_void()+
  labs(title = "Twitter text bigram")+
  theme(text = element_text(family = "Calibri"),
        plot.title = element_text(hjust = 0.5 , face = "bold", size = 18))
```

<div class="figure" style="text-align: center">

<img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-15-1.png" alt="Word bigram excludes Hebrew stop words and the word 'elections'" width="672" />
<p class="caption">
Figure 2: Word bigram excludes Hebrew stop words and the word â€˜electionsâ€™
</p>

</div>

<br>

**How should we read this graph?**

First off, We only plotted the 45 most common bigrams (out of 100,000+). Every word is connected to another word with an arrow pointing to a given direction. The direction to which the arrow points is the way to read that bigram. In addition, bolder lines represent a higher frequency of that bigram throughout all our text.  
For example, on the bottom of our graph we see the number â€˜2â€™ connected to the words â€˜mandatesâ€™ and â€˜campaginâ€™. The direction of the arrow signals that we should read the bigram as â€˜2 mandatesâ€™ and â€˜2 campaginsâ€™.

**What does this all mean?**

- We have discussions regarding the **number of chairs a govenrment will have (62/61/60/58)** connected to mentions of the number of election campaigns (2/3) we had, discussions of a united and/or minimal government and the forming of one in general.

- We see **mentions of individuals** such as â€œBenjamin Netanyahuâ€, â€œAmit Segalâ€ (Both we discussed earlier), â€œNatan Eshelâ€, **but no mention of the main candidate running against Netanyahu - â€œBenny Gantzâ€**. Thatâ€™s actually kind of odd, but more on that in a minute.  

- We also see mentions of political parties such as â€œMeretzâ€, â€œGesherâ€ and â€œLaborâ€ who ran together this time around, â€œOtzma Yehuditâ€, â€œUnited Torah Judaismâ€, and the â€œJoint Listâ€. **Thereâ€™s no mention of the two leading parties - â€œKahol Lavanâ€ & â€œThe Likkudâ€.**, despite the mentioning of the latterâ€™s leader.

- Mentions of Netanyahuâ€™s indicment and the personal law associated him.

- Mentions Iâ€™d categorize as â€˜otherâ€™ such as â€œTerrorist supportersâ€, â€œWill of the peopleâ€, â€œFake newsâ€, â€œGo voteâ€™, etc.
  <br>

Actaully, this turned out more interesting than I thought. Several questions arose while looking at it: Several words are missing such as the main parties names (Likkud & Kahol-Lavan), The leading oponent running against Benjamin Netanyahu - Benny Gantz - and other questions such as with whom are specific terms associated. Before we close up Iâ€™ll look at one question that troubles me - **Why doesnâ€™t Gantz appear in our list** ğŸ˜±?

#### Benny Gantzâ€™s disappearance

In order to see why Benny Gantz doesnâ€™t appear in our bigram plot Iâ€™ll do the following: Iâ€™ll break the text into bigrams and filter to **have only the bigrams containing the word Gantz**. Once we have that we can see why he doesnâ€™t appear in our bigram plot despite appearing in our wordcloud.  
Before I run the analysis and give you the answer think for a moment - What was the process of coming up with the bigram? If I chose only the 50 most frequent bigrams, why would a word that appears many times in our text not appear in our bigram list? Alternatively, did we filter anything along the way? Maybe even give the previous chunk another glance before I answer it.  
<br>
Letâ€™s have a look:

``` r
gantz <-elections %>%
  select(text) %>% 
  unnest_tokens(bigram, text, token = "ngrams", n = 2) %>%
  separate(bigram, into = c("word1", "word2"), sep = " ", remove = FALSE) %>% 
  filter(word1 %in% "×’× ×¥" |
         word2 %in% "×’× ×¥",
         !grepl("([a-z]+|×‘×—×™×¨×•×ª)", bigram))
```

The code is similar to what we did earlier only this time we left **bigrams that match the word we want** - bigrams containing the word Gantz. Now that we have our list of bigrams, letâ€™s look at the count of bigrams containing the word ×’× ×¥ (â€˜Gantzâ€™):

``` r
gantz %>% 
  count(bigram, sort = T)
```

    ## # A tibble: 955 x 2
    ##    bigram         n
    ##    <chr>      <int>
    ##  1 ×©×œ ×’× ×¥       160
    ##  2 ×‘× ×™ ×’× ×¥      138
    ##  3 ×’× ×¥ ×œ×        90
    ##  4 ×¢×œ ×’× ×¥        70
    ##  5 ××ª ×’× ×¥        69
    ##  6 ×¢× ×’× ×¥        61
    ##  7 ×× ×’× ×¥        41
    ##  8 ×’× ×¥ ×”×™×”       25
    ##  9 ×’× ×¥ ××•        19
    ## 10 ×’× ×¥ ×œ×™×‘×¨××Ÿ    19
    ## # ... with 945 more rows

**AHA!** Now I see what happened. The first bigram is a stop word and the word Gantz (â€˜Of Gantzâ€™). The second bigram should have been included as it is Gantzâ€™s full name - Benny Gantz, which appears 138 times.  
So, why has it been filtered? This is a great question which we can answer if we look at our stop words we initially used. Letâ€™s see if it has the word ×‘× ×™ (â€˜bennyâ€™ in Hebrew):

``` r
he_stopwords %>% 
  filter(word == "×‘× ×™")
```

    ## # A tibble: 1 x 1
    ##   word 
    ##   <chr>
    ## 1 ×‘× ×™

Yes it does. At the time of writing this blog post it leaves me in a dilemma - Should I change the stop words file I used to a different one or maybe create my own? Or should I continue as is? I think leaving it will teach me (and hopefully whoever read this far) a valuable lesson of always checking your stop words. In a different context the specific bigram wouldnâ€™t have got me thinking, but here it didnâ€™t make sense that our leading candidate was filtered, thus my inquire into what happened. In hebrew the word Benny also means â€˜my sonâ€™, which I wouldnâ€™t describe as a stop word but whoever made the dataset I guess did.

If you wish to give it a try yourself, you can find the data in the form of an `.rds` or smaller `.csv` (excludes list columns) in my [github repository](https://github.com/AmitLevinson/amitlevinson.com/blob/master/content/post/elections-twitter).

Well then, thatâ€™s all for now folks! **And remember, make sure to validate your stop words dataset!**
<br>

[^1]: The function `wordcloud2` we wrote wasnâ€™t actually run because it renders an html object which distorts the post. Instead I used the webshot of our rendered html file, read more about that [here](https://www.r-graph-gallery.com/196-the-wordcloud2-library.html).
