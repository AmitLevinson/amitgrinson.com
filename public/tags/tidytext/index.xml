<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>tidytext | Amit Levinson</title>
    <link>/tags/tidytext/</link>
      <atom:link href="/tags/tidytext/index.xml" rel="self" type="application/rss+xml" />
    <description>tidytext</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>Â© Amit Levinson 2020</copyright><lastBuildDate>Sun, 31 May 2020 00:00:00 +0000</lastBuildDate>
    <image>
      <url>/images/icon_hudc5a425b3e555d4faa8838526864126b_16079_512x512_fill_lanczos_center_2.png</url>
      <title>tidytext</title>
      <link>/tags/tidytext/</link>
    </image>
    
    <item>
      <title>Learning Tfidf with Political Theorists</title>
      <link>/post/learning-tfidf-with-political-theorists/</link>
      <pubDate>Sun, 31 May 2020 00:00:00 +0000</pubDate>
      <guid>/post/learning-tfidf-with-political-theorists/</guid>
      <description>
&lt;script src=&#34;/rmarkdown-libs/kePrint/kePrint.js&#34;&gt;&lt;/script&gt;


&lt;style&gt;
p.caption {
  font-size: 0.9em;
}
&lt;/style&gt;
&lt;div id=&#34;introduction&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Introduction&lt;/h3&gt;
&lt;p&gt;Thanks to &lt;a href=&#34;https://almogsi.com/&#34;&gt;Almog Simchon&lt;/a&gt; for insightful comments on a first draft of this post.&lt;/p&gt;
&lt;p&gt;Learning &lt;code&gt;R&lt;/code&gt; for the past nine months or so has enabled me to explore new topics that are of interest to me, one of them being text analysis. In this post Iâ€™ll explain what is Term-Frequency Inverse Document Frequency (tf-idf) and how it can help us explore important words for a document within a corpus of documents&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;. The analysis helps in finding words that are common in a given document but are rare across all other documents.&lt;/p&gt;
&lt;p&gt;Following the explanation weâ€™ll implement the method on four great philosophersâ€™ books: â€˜Republicâ€™ (Plato), â€˜The Princeâ€™ (Machiavelli), â€˜Leviathanâ€™ (Hobbes) and lastly, one of my favorite books - â€˜On Libertyâ€™ (Mill) ğŸ˜. Lastly, weâ€™ll see how tf-idf compares to a Bag of Words analysis (word count) and how using both can benefit your exploring of text.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;The post is aimed for anyone exploring text-analysis&lt;/strong&gt; and wants to learn about tf-idf. &lt;strong&gt;I will be using &lt;code&gt;R&lt;/code&gt; to analyze our data but wonâ€™t be explaining the different functions&lt;/strong&gt;, as this post focuses on the tf-idf analysis. If you wish to see the code, feel free to download or explore the .Rmd source code on my &lt;a href=&#34;https://github.com/AmitLevinson/amitlevinson.com/blob/master/content/post/learning-tfidf-with-political-theorists/index.Rmd&#34;&gt;github repository&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;term-frequency&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Term frequency&lt;/h3&gt;
&lt;p&gt;tf-idf gauges a wordâ€™s value according to two parameters: The first parameter is the &lt;strong&gt;term-frequency of a word: How common is a word in a given document&lt;/strong&gt; (Bag of Words analysis); one method to calculate term frequency of a word is just to count the total number of times each words appears. Another method - which weâ€™ll use in the tf-idf - is, after summing the total number of times a word appears, weâ€™ll divide it by the total number of words in that document, &lt;strong&gt;describing term frequency as such:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[tf = \frac{\textrm{Number of times a word appears in a document}}{\textrm{Total number of words in that document}}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Also written as &lt;span class=&#34;math inline&#34;&gt;\(tf(t,d)\)&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt; is the number of times a term appears out of all words in document &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt;. Using the above method weâ€™ll have the &lt;strong&gt;proportion&lt;/strong&gt; of each word in our document, a value ranging from 0 to 1, where common words will have higher values.&lt;/p&gt;
&lt;p&gt;While this gives us a value gauging how common a word is in a document, what happens when we have many words across many documents? How do we find &lt;ins&gt;unique&lt;/ins&gt; words for each document? This brings us to &lt;em&gt;idf&lt;/em&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;inverse-document-frequency&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Inverse document frequency&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Inverse document frequency accounts for the occurrence of a word across all documents, thereby giving a higher value to words appearing in less documents.&lt;/strong&gt; In this case, for each term we will calculate the log ratio&lt;a href=&#34;#fn2&#34; class=&#34;footnote-ref&#34; id=&#34;fnref2&#34;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt; of all documents divided by the number of documents that word appears in. This gives us the following:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ idf = \log {\frac{\textrm{N documents in corpus}}{\textrm{n documents containing the term}}}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Also written as &lt;span class=&#34;math inline&#34;&gt;\(idf = \log{\frac{N}{n(t)}}\)&lt;/span&gt; Where &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt; is the total number of documents in our corpus and &lt;span class=&#34;math inline&#34;&gt;\(n(t)\)&lt;/span&gt; is the number of documents the word appears within our corpus of documents.&lt;/p&gt;
&lt;p&gt;To those unfamiliar, a logarithmic transformation helps in reducing wide-ranged numbers to smaller scopes. In this case, if we have 7 documents, and our term appears in all 7 documents, weâ€™ll have following idf value: &lt;span class=&#34;math inline&#34;&gt;\(log_e(\frac{7}{7}) = 0\)&lt;/span&gt;. What if we have a term that appears in only 1 document out of all 7 documents? Weâ€™ll have the following: &lt;span class=&#34;math inline&#34;&gt;\(log_e(\frac{7}{1}) = 1.945\)&lt;/span&gt;. Even if a word appears in only 1 document out of 100, a logarithmic transformation will reduce its high value to mitigate bias when we multiply it with its &lt;span class=&#34;math inline&#34;&gt;\(tf\)&lt;/span&gt; value.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;So what do we understand from the idf?&lt;/strong&gt; Since our numerate always remains the same (N documents in corpus), the &lt;em&gt;idf&lt;/em&gt; of a word is contingent upon how common it is &lt;em&gt;across&lt;/em&gt; documents. Words that appear in a small number of documents will have a higher &lt;em&gt;idf&lt;/em&gt;, while words that are common across documents will have a lower &lt;em&gt;idf&lt;/em&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;term-frequency-inverse-document-frequency-tfidf&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Term-Frequency Inverse Document Frequency (tfidf)&lt;/h3&gt;
&lt;p&gt;Once we have the term frequency and inverse document frequency for each word we can calculate the tf-idf by multiplying the two: &lt;span class=&#34;math inline&#34;&gt;\(tf(t,d) \cdot idf(t,D)\)&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(D\)&lt;/span&gt; is our corpus of documents.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;To summarize our explanation:&lt;/strong&gt; The two paramteres used to calculate the tf-idf provide each word with a value for its importance to that document in that corpus of text. Ideally We take &lt;strong&gt;words that are &lt;u&gt;common within a document&lt;/u&gt; and that are &lt;u&gt;rare across documents&lt;/u&gt;&lt;/strong&gt;. I write ideally because as weâ€™ll see soon, we might have words that are extremely common in one document but are filtered out because theyâ€™re evident in all documents (can happen in a small corpus of documents). This also highlights the question as to what is &lt;em&gt;important&lt;/em&gt;; I define important as contributing to understanding a document in comparison to all other documents.&lt;/p&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: center&#34;&gt;&lt;span id=&#34;fig:unnamed-chunk-2&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;/post/learning-tfidf-with-political-theorists/index_files/figure-html/unnamed-chunk-2-1.png&#34; alt=&#34;Using tf-idf we can calculate how common a word is within a document and how rare is it across documents&#34; width=&#34;85%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 1: Using tf-idf we can calculate how common a word is within a document and how rare is it across documents
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Now that we have some background as to how tf-idf works, letâ€™s dive in to our case study.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;tf-idf-on-political-theorists.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;TF-IDF on political theorists.&lt;/h3&gt;
&lt;p&gt;Iâ€™m a big fan of political theory. I have a small collection at home and always like to read and learn more about it. Except for Mill, we read Plato, Machiavelli and Hobbes in our BA first semester course in political theory. While some of the theorists overlap to some degree, over-all they discuss different topics. tf-idf will help us distinguish important words specific to each book, in a comparison across all books.&lt;/p&gt;
&lt;p&gt;Before we conduct our tf-idf weâ€™d like to explore our text a bit. The following exploratory analysis is inspired from Julia Silgeâ€™s blog post &lt;a href=&#34;(https://www.r-bloggers.com/term-frequency-and-tf-idf-using-tidy-data-principles/)&#34;&gt;â€˜Term Frequency and tf-idf Using Tidy Data Principlesâ€™&lt;/a&gt;, a fantastic read.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;data-collection-analysis&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Data collection &amp;amp; Analysis&lt;/h3&gt;
&lt;p&gt;The package weâ€™ll use to gather the data is the &lt;code&gt;{gutenbergr}&lt;/code&gt; package. It enables us to access the &lt;a href=&#34;https://www.gutenberg.org/&#34;&gt;Project Gutenberg&lt;/a&gt; free books, a library of over 60,000 free books. As many other amazing things in &lt;code&gt;R&lt;/code&gt; someone, in this case David Robinson, created a package for it. All we need to do is download them to our computer.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;Mill &amp;lt;- gutenberg_download(34901)
Hobbes &amp;lt;- gutenberg_download(3207)
Machiavelli &amp;lt;- gutenberg_download(1232)
Plato &amp;lt;- gutenberg_download(150)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Several of the books contain sections at the beginning or at the end that arenâ€™t relevant for our analysis. For example long introductions from contemporary scholars; another whole different book at the end, etc. These can confound our analysis and therefore weâ€™ll exclude them. In order to conduct our analysis we also need all the books we collected in one object.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Once we are able to clean the books, this is what our text looks like:&lt;/strong&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;remove_text &amp;lt;- function(book, low_id, top_id = max(rowid), author = deparse(substitute(book))){
  book %&amp;gt;%
  mutate(author = as.factor(author)) %&amp;gt;% 
  rowid_to_column() %&amp;gt;% 
  filter(rowid &amp;gt;= {{low_id}}, rowid &amp;lt;= {{top_id}}) %&amp;gt;% 
  select(author, text, -c(rowid, gutenberg_id))}

books &amp;lt;- rbind(
  remove_text(Mill, 454),
  remove_text(Hobbes, 360, 22317),
  remove_text(Machiavelli, 464, 3790),
  remove_text(Plato, 606))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 45,490 x 2
##    author text                                                                  
##    &amp;lt;fct&amp;gt;  &amp;lt;chr&amp;gt;                                                                 
##  1 Mill   &amp;quot;&amp;quot;                                                                    
##  2 Mill   &amp;quot;&amp;quot;                                                                    
##  3 Mill   &amp;quot;CHAPTER I.&amp;quot;                                                          
##  4 Mill   &amp;quot;&amp;quot;                                                                    
##  5 Mill   &amp;quot;INTRODUCTORY.&amp;quot;                                                       
##  6 Mill   &amp;quot;&amp;quot;                                                                    
##  7 Mill   &amp;quot;&amp;quot;                                                                    
##  8 Mill   &amp;quot;The subject of this Essay is not the so-called Liberty of the Will, ~
##  9 Mill   &amp;quot;unfortunately opposed to the misnamed doctrine of Philosophical&amp;quot;     
## 10 Mill   &amp;quot;Necessity; but Civil, or Social Liberty: the nature and limits of th~
## # ... with 45,480 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Each row is some text with chapters separated by headings and a column referencing who is the author. Our data frame consists of ~45,000 rows with the filtered text from our four books. Tf-idf can also be done on any n-grams we choose (number of consequent words). We could calculate the tf-idf for each bigram of words (two-words), trigram, etc. I find a unigram an appropriate approach both for tf-idf and especially now when we want to learn more about it. &lt;strong&gt;We just saw that our text is in the form of sentences, so letâ€™s break it into single words.&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 12 x 4
## # Groups:   author [4]
##    author      word      n sum_words
##    &amp;lt;fct&amp;gt;       &amp;lt;chr&amp;gt; &amp;lt;int&amp;gt;     &amp;lt;int&amp;gt;
##  1 Hobbes      the   14536    207849
##  2 Hobbes      of    10523    207849
##  3 Hobbes      and    7113    207849
##  4 Plato       the    7054    118639
##  5 Plato       and    5746    118639
##  6 Plato       of     4640    118639
##  7 Mill        the    3019     48006
##  8 Mill        of     2461     48006
##  9 Machiavelli the    2006     34821
## 10 Mill        to     1765     48006
## 11 Machiavelli to     1468     34821
## 12 Machiavelli and    1333     34821&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We see that stop-words dominant the frequency of occurrences. That makes sense as they are commonly used, but theyâ€™re not usually helpful for learning about a text, specifically here. &lt;strong&gt;Weâ€™ll start by exploring how the word frequencies occur within a text:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/learning-tfidf-with-political-theorists/index_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;672&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The plot above shows the frequency of terms across documents. We see some words that appear frequently (higher proportion = right side of the x-axis) and many words that are rarer (low proportion). Actually, I had to limit the x-axis or otherwise it would distort the plot with words that are extremely common.&lt;/p&gt;
&lt;p&gt;To help find useful words with the highest tf-idf from each book, weâ€™ll remove stop words before we extract the words with a high tf-idf value:&lt;/p&gt;
&lt;table class=&#34;table&#34; style=&#34;width: auto !important; margin-left: auto; margin-right: auto;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
Author
&lt;/th&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
Word
&lt;/th&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
n
&lt;/th&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
Sum words
&lt;/th&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
Term Frequency
&lt;/th&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
IDF
&lt;/th&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
TF-IDF
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
Mill
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
opinion
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
150
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
48006
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.0094132
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.0000000
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.0000000
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
Hobbes
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
god
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
1047
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
207849
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.0149024
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.0000000
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.0000000
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
Machiavelli
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
prince
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
185
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
34821
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.0172704
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.2876821
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.0049684
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
Plato
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
true
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
485
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
118639
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.0152953
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.0000000
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.0000000
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;tfoot&gt;
&lt;tr&gt;
&lt;td style=&#34;padding: 0; border: 0;&#34; colspan=&#34;100%&#34;&gt;
&lt;sup&gt;&lt;/sup&gt; Random sample of words and their corresponding tf-idf values
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tfoot&gt;
&lt;/table&gt;
&lt;p&gt;Above we have our tf-idf for a given word from each document. I removed stop-words and calculated the tf-idf for each word in each book. For Hobbes the word â€˜Godâ€™ appears 1047 times, thus has a &lt;span class=&#34;math inline&#34;&gt;\(tf\)&lt;/span&gt; of &lt;span class=&#34;math inline&#34;&gt;\(\frac {1047} {207849}\)&lt;/span&gt; and an idf of 0 (since it appears in all documents), so itâ€™ll have a tf-idf of 0.&lt;/p&gt;
&lt;p&gt;With Machiavelli the word prince appears 185 times, with a &lt;span class=&#34;math inline&#34;&gt;\(tf\)&lt;/span&gt; of &lt;span class=&#34;math inline&#34;&gt;\(\frac {185} {34821}\)&lt;/span&gt;, resulting in a proportion of 0.0173. The word prince has an idf of 0.288 &lt;span class=&#34;math inline&#34;&gt;\((log_e(\frac 4 {3}))\)&lt;/span&gt;, as there are 4 documents and it appears in 3 of them, so a total tf-idf value of &lt;span class=&#34;math inline&#34;&gt;\(0.0173 \cdot 0.288\)&lt;/span&gt; = &lt;span class=&#34;math inline&#34;&gt;\(0.00497\)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;tf-idf-plot&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Tf-idf plot&lt;/h3&gt;
&lt;p&gt;As we wrap up our tf-idf analysis, &lt;strong&gt;We donâ€™t want to see all words and their tf-idf, but only words with the highest tf-idf value&lt;/strong&gt; for each author, indicating the importance of a word to a given document. We can look at these words by plotting the top 10 highest valued tf-idf words for each author:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt; ggplot(data = books_for_plot, aes(x = word, y = tf_idf, fill = author))+
  geom_col(show.legend = FALSE)+
  labs(x = NULL, y = &amp;quot;tf-idf&amp;quot;)+
  coord_flip()+
  scale_x_reordered()+
  facet_wrap(~ author, scales = &amp;quot;free_y&amp;quot;, ncol = 2)+
  labs(title = &amp;quot;&amp;lt;b&amp;gt;Term Frequency Inverse Document Frequency&amp;lt;/b&amp;gt; - Political theorists&amp;quot;,
       subtitle = &amp;quot;tf-idf for The Leviathan (Hobbes), On Liberty (Mill), The Prince (Machiavelli)\nand Republic (Plato)&amp;quot;)+
  scale_fill_manual(values = plot_colors)+
  theme_post+
  theme(plot.title = element_markdown())&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/learning-tfidf-with-political-theorists/index_files/figure-html/unnamed-chunk-11-1.png&#34; width=&#34;672&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Lovely!&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Letâ€™s review each book and see what we can learn from our tf-idf analysis. My memory of these books is kind of rusty but Iâ€™ll try my best:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Hobbes:&lt;/strong&gt; Hobbes in his book describes the &lt;em&gt;natural&lt;/em&gt; state of human beings and how they can leave it by revoking many of their right to the &lt;em&gt;sovereign&lt;/em&gt; who will facilitate order. In his book he describes the soveragin (note the â€˜aâ€™) as needed to be strict, rigorous and &lt;em&gt;hath&lt;/em&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Machiavelli:&lt;/strong&gt; Machiavelli provides a leader with a guide on how to rule his country. He prefaces his book with an introduction letter to the &lt;em&gt;Duke&lt;/em&gt;, the recipient of his work. Machiavelli throughout the book conveys his message with examples of many &lt;em&gt;princes&lt;/em&gt;, &lt;em&gt;Alexander&lt;/em&gt; the great, the &lt;em&gt;Orsini&lt;/em&gt; brothers and more. Several of his examples include mentioning of Italy (where he resides), specifically &lt;em&gt;Venetians&lt;/em&gt; and &lt;em&gt;Milan&lt;/em&gt;.&lt;br /&gt;
&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Mill:&lt;/strong&gt; Mill in his book â€˜On Libertyâ€™ describes the importance of freedom and liberty for individuals. He does so by describing the relation between people and their &lt;em&gt;society&lt;/em&gt; and other relations with the &lt;em&gt;social&lt;/em&gt;. He highlights in his discussion on liberty a &lt;em&gt;personâ€™s&lt;/em&gt; belonging; these can be &lt;em&gt;Feelings&lt;/em&gt; or basically anything &lt;em&gt;personal&lt;/em&gt;. Protecting the personal is important for the &lt;em&gt;development&lt;/em&gt; of both society and that of the individual.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Plato:&lt;/strong&gt; Platoâ€™s book consists of 10 chapters and it is by far the longest compared to the others. The book is written in the form of a dialogue with &lt;em&gt;replies&lt;/em&gt; between Socrate and his discussants. Along Socrateâ€™s journey to finding out what is the meaning of justice he talks to many people, among them &lt;em&gt;Glaucon&lt;/em&gt;, &lt;em&gt;Thrasymachus&lt;/em&gt; and &lt;em&gt;Adeimantus&lt;/em&gt;. In one section Socrates describes a just society with distinct &lt;em&gt;classes&lt;/em&gt; such as the &lt;em&gt;guardians&lt;/em&gt;. The classes should receive appropriate education, for e.g.Â &lt;em&gt;gymnastics&lt;/em&gt; for the guardians.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;With the above analysis we were able to explore uniqueness of words for each book across all books. &lt;strong&gt;Some words provided us with great insights while others didnâ€™t necessarily help us despite their uniqeness&lt;/strong&gt;, for example, the names of discussants with Socrate. Tf-idf gauges them as important (as to how I defined importance here) to distinguish between Platoâ€™s book and the others, but Iâ€™m sure theyâ€™re not the first words that come to mind when someone talks about the Republic.&lt;/p&gt;
&lt;p&gt;The analysis also shows this methodologyâ€™s value addition is not in just applying tf-idf - or any other statistical analysis â€“ rather its power lies in its explanatory abilities. In other words, &lt;strong&gt;tf-idf provides us with a value indicating the importance of a word to a given document within a corpus, it is our job to take that extra step interpreting and contextualizing the output.&lt;/strong&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;comparing-to-bag-of-words-bog&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Comparing to Bag Of Words (BOG)&lt;/h3&gt;
&lt;p&gt;A common text analysis is a word count I discussed earlier, also known as Bag of Words (BoW). This is an easy to understand method that can be done easily when exploring text. However, relying only on a bag of words method to draw insights can limit its usefulness if other analytic methods are not also included. The BoW relies only on the frequency of a word, so if a word is common across all documents, it might show up in all of them and not contribute to finding &lt;em&gt;unique words&lt;/em&gt; for each document.&lt;/p&gt;
&lt;p&gt;Now that we have our books we can also explore the raw occurrence of each word to compare it to our above tf-idf analysis:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(data = bow_books, aes(x = reorder(word_with_color,n), y = n, fill = author))+
  geom_col(show.legend = FALSE)+
  labs(x = NULL, y = &amp;quot;Word Frequency&amp;quot;)+
  coord_flip()+
  scale_x_reordered()+
  facet_wrap(~ author, scales = &amp;quot;free&amp;quot;, ncol = 2)+
  labs(title = &amp;quot;&amp;lt;b&amp;gt;Term Frequency&amp;lt;/b&amp;gt; - Political theorists&amp;quot;)+
  scale_fill_manual(values = plot_colors)+
  theme_post+
  theme(axis.text.y = element_markdown(),
        plot.title = element_markdown(),
        strip.text = element_text(color = &amp;quot;grey50&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: center&#34;&gt;&lt;span id=&#34;fig:unnamed-chunk-13&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;/post/learning-tfidf-with-political-theorists/index_files/figure-html/unnamed-chunk-13-1.png&#34; alt=&#34;Term frequency plot with words that are common across documents in bold&#34; width=&#34;672&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 2: Term frequency plot with words that are common across documents in bold
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;The above plot amplifies, in my opinion, tf-idfâ€™s contribution in finding unique words for each document.&lt;/strong&gt; While many of the words are similar to those we found in the previous tf-idf analysis, we also draw words that are common across documents. For example, we see the frequency of â€˜Timeâ€™, â€˜Peopleâ€™ and â€˜Natureâ€™ twice in different books and words such as â€˜Trueâ€™ and â€˜Truthâ€™ with similar meanings do so too (however this could have happened in tf-idf too).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;However, the Bag of Words also provided new words we didnâ€™t see earlier.&lt;/strong&gt; Here we can learn on new words like Power in Hobbes, Opinions in Mill and more. With the bag of words we get words that are common without controlling for other texts, while the tf-idf searches for words that are common within but are rare across.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;closing-remarks&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Closing remarks&lt;/h3&gt;
&lt;p&gt;In this post we learned the term frequency inverse document frequency (tf-idf) analysis and implemented it on four great political theorists. We finished by exploring tfidf in comparison to a bag of words analysis and showed the benefits of each. This also emphasizes how we define &lt;em&gt;important&lt;/em&gt;: Important to a document by itself or important to a document compared to other documents.
The definition of â€˜importantâ€™ here also highlights tf-idf heuristic quantifying approach (&lt;a href=&#34;https://en.wikipedia.org/wiki/Tf%E2%80%93idf&#34;&gt;specifically the idf&lt;/a&gt;) and thus should be used with caution. If you are aware of theoretical development of it Iâ€™d be glad to read more about it.&lt;/p&gt;
&lt;p&gt;By now you should be equipped to give tf-idf a try yourself on a corpus of documents you find appropriate.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;where-to-next&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Where to next&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Further reading about text analysis - If you want to read more on text mining with R, I highly recommend the Julia Silge &amp;amp; David Robinsonâ€™s &lt;a href=&#34;https://www.tidytextmining.com/&#34;&gt;text mining with R book&lt;/a&gt;and/or exploring the &lt;a href=&#34;https://quanteda.io/&#34;&gt;&lt;code&gt;{quanteda}&lt;/code&gt;&lt;/a&gt; package.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Text datasets - As to finding text data, you can try the &lt;code&gt;{gutenbergr}&lt;/code&gt; package that gives access to thousands of books, a &lt;a href=&#34;https://github.com/rfordatascience/tidytuesday&#34;&gt;#TidyTuesday&lt;/a&gt; data set or collect tweets from Twitter using the &lt;code&gt;{rtweet}&lt;/code&gt; package.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Other posts of mine - If youâ€™re interested in other posts of mine where I explore some text you can read my &lt;a href=&#34;https://amitlevinson.com/2020/04/20/israeli-elections-on-twitter/&#34;&gt;Israeli elections Twitter tweets analysis&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Thatâ€™s it for now. Feel free to contact me for any and all comments!&lt;/p&gt;
&lt;div id=&#34;notes&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Notes&lt;/h4&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;A single document can be a book, chapter, paragraph or sentence, it all depends on your research and what you define as an â€˜entityâ€™ within a corpus of text.&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;â†©&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn2&#34;&gt;&lt;p&gt;&lt;strong&gt;Whatâ€™s log ratio?&lt;/strong&gt;: Considering I was only exposed to &lt;span class=&#34;math inline&#34;&gt;\(log\)&lt;/span&gt; (short for logarithm) late in my academic studies, I thought I might as well briefly explain it to those unfamiliar with it: In general, and for the purpose of the tf-idf, a logarithm transformation helps in reducing wide ranged numbers to smaller scopes. Assuming we have the following &lt;span class=&#34;math inline&#34;&gt;\(\log _{2}(16) = x\)&lt;/span&gt;, we ask ourselves (and calculate) 2 in the power of what (x) will give us 16. so in this case 2^3 will give us 16, which is basically written as &lt;span class=&#34;math inline&#34;&gt;\(\log _{2}(16) = 3\)&lt;/span&gt;. In order to generalize it, &lt;span class=&#34;math inline&#34;&gt;\(\log _{b}(x) = y\)&lt;/span&gt;, means b is the base we will raise to the power of y to reach x. Therefore written oppositely as &lt;span class=&#34;math inline&#34;&gt;\(b^y = x\)&lt;/span&gt;. The common uses of log are &lt;span class=&#34;math inline&#34;&gt;\(\log_2\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\log_{10}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(log_e\)&lt;/span&gt;, also written as plain log.&lt;a href=&#34;#fnref2&#34; class=&#34;footnote-back&#34;&gt;â†©&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Israeli elections on Twitter</title>
      <link>/post/israeli-elections-on-twitter/</link>
      <pubDate>Mon, 20 Apr 2020 00:00:00 +0000</pubDate>
      <guid>/post/israeli-elections-on-twitter/</guid>
      <description>


&lt;div id=&#34;introduction&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Introduction&lt;/h3&gt;
&lt;p&gt;Israel had its 3rd election within 12 months on March 2, 2020. This is because our Knesset - Hebrew term for house of representatives - wasnâ€™t able to form or hold a government after each of the previous elections. As I wonâ€™t get into the politics of why they didnâ€™t succeed in forming one (get it? politics ğŸ˜‰), I do want to take the opportunity and analyze some tweets posted in the time before and after the elections.&lt;br /&gt;
When we think of a data aggregating tweets, many questions arise - who, what, when, where and more about our data. Namely, with the collected data I want to answer the following questions:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;What was the frequency of tweets associated with the word â€˜electionsâ€™?&lt;/li&gt;
&lt;li&gt;Who tweeted the most?&lt;/li&gt;
&lt;li&gt;What was the most common #Hashtag tweeted?&lt;/li&gt;
&lt;li&gt;Which tweet was most liked and which was retweeted the most?&lt;/li&gt;
&lt;li&gt;What were the most common words and bigrams (two words) in tweets?&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;gathering-the-data&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Gathering the data &lt;i class=&#34;fab fa-twitter&#34;&gt;&lt;/i&gt;&lt;/h3&gt;
&lt;p&gt;Twitterâ€™s API allows scraping &lt;strong&gt;6-9 days back for free&lt;/strong&gt;. Therefore, I scraped the data already on March 7, 2020 and saved it for later use.&lt;/p&gt;
&lt;p&gt;Letâ€™s start with the packages weâ€™ll use:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(rtweet)
library(tidyverse)
library(tidytext)
library(igraph)
library(hrbrthemes)
library(ggraph)
library(extrafont)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I could use a consistent plot theme throughout the post but Iâ€™ll probably be editing each one a bit, while also some are not our regular graphs. With that said, There are some tweaks that will be consistent acorss several of the plots. Therefore, letâ€™s create a theme function as a supplement to all other theme arguments Iâ€™ll use that will save a few lines of code:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mini_theme &amp;lt;- function(family = &amp;quot;Roboto Condensed&amp;quot;, tsize = 16) {
  theme_classic() +
  theme(text = element_text(family = family),
        axis.ticks = element_blank(),
        axis.line = element_blank(),
        plot.title = element_text(size = tsize))}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next weâ€™ll gather the tweets we need:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;elections_raw &amp;lt;- search_tweets(&amp;quot;×‘×—×™×¨×•×ª&amp;quot;, n = 18000, retryonratelimit = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To gather the tweets we can use the &lt;a href=&#34;https://rtweet.info/&#34;&gt;{rtweet}&lt;/a&gt; package which is amazing for collecting Twitter data. As I mentioned earlier, I already scraped the data a few days after the elections but left the command here to show what we did and how easy it is to do it. I searched only one term, â€˜electionsâ€™ in Hebrew, and rtweet gathered all tweets containing that word.&lt;/p&gt;
&lt;p&gt;What did our search yield? Letâ€™s have a look:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dim(elections)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 16560    90&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;16,560 rows and 90 columns! As we can see, the &lt;code&gt;{rtweet}&lt;/code&gt; package brings back a lot of information!&lt;/p&gt;
&lt;div id=&#34;some-caveats&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Some Caveats:&lt;/h4&gt;
&lt;p&gt;Before we begin, I will say this post doesnâ€™t aim to be representative of the discussions that were held during the election period. As a matter of fact, nor does it aim to be representative of the twitter discussions surrounding the elections. this is due to two main reasons:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Twitter isnâ€™t common in Israel at all. Iâ€™m not sure whatâ€™s the usage rate but itâ€™s definitely not representative of the Israeli population.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;I searched for only one word - elections (in Hebrew) - which yielded some 16560 tweets. This is definitely not a large enough pool of tweets to claim for representation.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;With that said, the data gathered provides an opportunity to look at some Twitter data from the election period and motivate others to use the &lt;code&gt;{rtweet}&lt;/code&gt; package, so why not give it a go.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;tweet-frequency&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Tweet frequency&lt;/h3&gt;
&lt;p&gt;First, letâ€™s see how the tweets distribute across the time span we searched for. we can create a quick time plot using the &lt;code&gt;ts_plot()&lt;/code&gt; argument from the &lt;code&gt;{rtweet}&lt;/code&gt; package:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;elections %&amp;gt;% 
  ts_plot(&amp;quot;2 hours&amp;quot;)+
  geom_line(size = 1, color = &amp;quot;black&amp;quot;)+
  mini_theme()+
  scale_x_datetime(date_breaks = &amp;quot;1 day&amp;quot;,date_labels = &amp;quot;%d %b&amp;quot;)+
  labs(x= NULL, y = NULL,
       title = &amp;quot;Tweet frequency throughout the Israeli elections week&amp;quot;,
       subtitle = &amp;quot;Tweets aggregated by two-hour interval. Only tweets containing the word &amp;#39;elections&amp;#39;\nin Hebrew were gathered&amp;quot;)+
  geom_text(aes(x = as.POSIXct(&amp;quot;2020-03-02 23:00:00&amp;quot;), y = 435, label = &amp;quot;10 PM:\nPolls close&amp;quot;),
            hjust = 0, size = 3, family = &amp;quot;Roboto Condensed&amp;quot;)+
  geom_vline(xintercept = as.POSIXct(&amp;quot;2020-03-02 22:00&amp;quot;),linetype = &amp;quot;dashed&amp;quot;, size = 0.5, color = &amp;quot;black&amp;quot;, alpha = 5/10)+
  theme(plot.subtitle = element_text(color = &amp;quot;gray70&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/elections-twitter/index_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;672&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Interesting - we see the number of tweets during the closing time is equivalent to that of midday on March 4th. Most of the votes were counted by the end of March 3rd, so I canâ€™t really put my finger on what this jump represents. After all, I collected tweets containing our word so it could have been that many people tweeted that specific term in that time slot. Anyway, I wasnâ€™t able to find anything interesting that happened on the news that day but feel free to explore and offer suggestions.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;users-with-most-tweets&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Users with most tweets&lt;/h3&gt;
&lt;p&gt;Next, letâ€™s look at who tweeted the most:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;elections %&amp;gt;% 
  count(screen_name, sort = T) %&amp;gt;% 
  slice(1:15) %&amp;gt;% 
  mutate(screen_name = reorder(screen_name,n)) %&amp;gt;% 
  ggplot(aes(x= screen_name, y= n))+
  geom_col(fill = &amp;quot;gray70&amp;quot;)+
  coord_flip()+
  scale_y_continuous(breaks = seq(0,180, 30), labels = seq(0,180,30))+
  labs(x = &amp;quot;Screen name&amp;quot;, y = &amp;quot;Number of tweets&amp;quot;, title = &amp;quot;Top 15 users tweeting the word &amp;#39;elections&amp;#39;&amp;quot;)+
  mini_theme()+
  theme(text = element_text(family = &amp;quot;Calibri&amp;quot;),
        axis.text = element_text(size = 12),
        axis.title.y = element_blank())&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/elections-twitter/index_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;768&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We see that many news companies tweeted a lot using the word â€˜electionsâ€™: â€˜newisrael13â€™, â€˜kann_newsâ€™, â€˜MaarivOnlineâ€™, â€˜RotterNewsâ€™, â€˜bahazit_newsâ€™, â€˜RotterNetâ€™. I personnaly donâ€™t recognize the rest, but on the other hand I use Twitter mostly to follow &lt;code&gt;R&lt;/code&gt; and academic related tweets, not necessarily Israeli politics.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;common-hashtags&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Common Hashtags&lt;/h3&gt;
&lt;p&gt;When using the &lt;code&gt;{rtweet}&lt;/code&gt; package to gather twitter data, one of the variables collected is the hashtags used in tweets. Although it doesnâ€™t require too many lines of code to extract hashtags out of text, I think this is an amazing feature that shows the effort and details &lt;a href=&#34;https://mikewk.com/&#34;&gt;Michael W. Kearney&lt;/a&gt; and contributors put into the package.&lt;/p&gt;
&lt;p&gt;According to &lt;a href=&#34;https://en.wikipedia.org/wiki/Hashtag&#34;&gt;Wikipedia&lt;/a&gt;, a â€˜Hashtagâ€™ â€œis a type of metadata tag used on social networks such as Twitter and other microblogging services.â€, that basically tags the message with a specific theme. This helps to see trends and themes in a macro level.&lt;/p&gt;
&lt;p&gt;OK then, letâ€™s see what we have:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;hashtags &amp;lt;- elections %&amp;gt;% 
  select(hashtags) %&amp;gt;% 
  unlist() %&amp;gt;% 
  as.tibble() %&amp;gt;% 
  mutate(value = tolower(value)) %&amp;gt;% 
  count(value, name = &amp;quot;Count&amp;quot;, sort = T) %&amp;gt;%
  mutate(value = reorder(value, Count),
         iscorona = ifelse(value == &amp;quot;×§×•×¨×•× ×”&amp;quot; | value == &amp;quot;coronavirus&amp;quot;, &amp;quot;y&amp;quot;, &amp;quot;n&amp;quot;)) %&amp;gt;% 
  filter(!is.na(value)) %&amp;gt;% 
  slice(1:20)

ggplot(data = hashtags, aes(x = Count, y = value, fill = iscorona))+
  geom_col(show.legend = FALSE)+
  scale_fill_manual(values = c(y = &amp;quot;#1DA1F2&amp;quot;, n = &amp;quot;gray70&amp;quot;))+
  labs(y = NULL, x = &amp;quot;Number of Tweets&amp;quot;, title = &amp;quot;Top 20 Hashtags addressing the Israeli elections&amp;quot;)+
  mini_theme()+
  theme(text = element_text(family = &amp;quot;Calibri&amp;quot;),
        axis.text = element_text(size = 12))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/elections-twitter/index_files/figure-html/unnamed-chunk-9-1.png&#34; width=&#34;672&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The tweets include pretty much what we expect - hashtags about the elections - with the two leading ones being â€˜electionsâ€™ and â€˜elections2020â€™. We also see a peculiar hashtag â€˜right_following_right_peopleâ€™, and others such as â€˜Netanyahuâ€™ (the Prime minister at the time), â€˜Israelâ€™ and others.&lt;br /&gt;
I highlighted in blue an interesting hashtag at the time - &lt;font color=&#34;#1DA1F2&#34;&gt;&lt;strong&gt;Corona&lt;/strong&gt;&lt;/font&gt; (in hebrew) and &lt;font color=&#34;#1DA1F2&#34;&gt;&lt;strong&gt;coronavirus&lt;/strong&gt;&lt;/font&gt;. The elections were held on March 2, 2020, a little bit after the first cases reached Israel. Little did we know how it will affect us (Iâ€™m finalzing this post on April 18, 2020, and only now weâ€™re starting to get back to routine. Slowly)&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;most-liked-and-retweeted&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Most liked and retweeted&lt;/h3&gt;
&lt;p&gt;Letâ€™s have a look at which tweet was &lt;strong&gt;most liked&lt;/strong&gt;. Twitter doesnâ€™t define it as â€˜likesâ€™ but as â€˜favoriteâ€™, or at least in the data that is collected through the &lt;code&gt;{rtweet}&lt;/code&gt; package. Since I will want to gather the most of something - both favorite and later retweeted - Iâ€™ll create a function that will minimize re-writing the code.&lt;br /&gt;
&lt;br&gt;
The function takes in a variable, reorders our dataset according to the variable we declared, extracts the first row and then pulls (extracts) the status id of that tweet. Lastly, the &lt;code&gt;blogdown::shortcode&lt;/code&gt; enables to embed tweets, youtube videos and more into a blogdown post such as this, so we end the function by inserting our status id into that. For those just getting into functions notice that within the &lt;code&gt;arrange&lt;/code&gt; argument we insert our variable in two curly brackets {{}}. This is a powerful feature of &lt;code&gt;{rlang}&lt;/code&gt; when you want to manipulate a variable in a dataframe within a function. You can read more about that &lt;a href=&#34;https://www.tidyverse.org/blog/2019/06/rlang-0-4-0/&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;get_most &amp;lt;- function(var){
elections %&amp;gt;% 
  arrange(desc({{var}})) %&amp;gt;% 
  .[1,] %&amp;gt;% 
  pull(status_id) %&amp;gt;% 
  blogdown::shortcode(&amp;#39;tweet&amp;#39;,.)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now Letâ€™s see which tweet was &lt;strong&gt;most liked&lt;/strong&gt; during that week:&lt;/p&gt;
&lt;center&gt;
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;iw&#34; dir=&#34;rtl&#34;&gt;×™×•×ª×¨ ××”×›×œ, ×× ×™ ×©××— ×©×œ× ×™×”×™×• ×¢×•×“ ×‘×—×™×¨×•×ª ×‘×©×‘×™×œ ×”××©×¤×—×” ×©×œ×™ ×©×¡×‘×œ×” ×‘×’×‘×•×¨×” ×©× ×” ×•×¨×‘×¢. ×¨×¢×•×ª ×¢×‘×¨×™ ×•×¢× ×¨ ğŸ˜&lt;/p&gt;&amp;mdash; ×¢××™×ª ×¡×’×œ Amit Segal (@amit_segal) &lt;a href=&#34;https://twitter.com/amit_segal/status/1234584864415997952?ref_src=twsrc%5Etfw&#34;&gt;March 2, 2020&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;

&lt;/center&gt;
&lt;p&gt;The tweet is by â€˜Amit Segalâ€™ - an Israeli news reporter - and it says (my translation):&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;â€œMore than anything, Iâ€™m glad there wonâ€™t be another elections for my family that suffered in honors a year and a quarter. Reut, Ivri and Aner ğŸ˜â€&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Ha, interestingly he wrote it before the end of the elections, hopefully heâ€™s right!&lt;/p&gt;
&lt;p&gt;Now letâ€™s look at the &lt;strong&gt;most re-tweeted&lt;/strong&gt; tweet:&lt;/p&gt;
&lt;center&gt;
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;iw&#34; dir=&#34;rtl&#34;&gt;×× ×”×”×§×œ×˜×” ×©×œ ×™×•×¢×¦×• ×©×œ ×’× ×¥ ××‘×•×©×œ×ª ×•×©×§×¨×™×ª (×›×“×‘×¨×™ ×’× ×¥ ×¢×›×©×™×•), ××– ×œ××” ×’× ×¥ ×¤×™×˜×¨ ××•×ª×•?&lt;br&gt;&lt;br&gt;×™×•×¢×¦×• ×©×œ ×’× ×¥ ×¤×•×˜×¨ ×‘×’×œ×œ ×©×××¨ ××ª ×”×××ª ×©×›×•×œ× ×™×•×“×¢×™×: ×’× ×¥ ×œ× ×™×›×•×œ ×œ×”×™×•×ª ×¨××© ×××©×œ×”. ×× ×—× ×• ×›×Ÿ. ×¢×•×“ 2 ×× ×“×˜×™× ×œ×œ×™×›×•×“ ×•×× ×—× ×• ××•×¦×™××™× ××ª ×”××“×™× ×” ××”×¤×œ×•× ×˜×¨, ××•× ×¢×™× ×¢×•×“ ×‘×—×™×¨×•×ª ×•××§×™××™× ×××©×œ×”&lt;/p&gt;&amp;mdash; Benjamin Netanyahu (@netanyahu) &lt;a href=&#34;https://twitter.com/netanyahu/status/1233342393740603394?ref_src=twsrc%5Etfw&#34;&gt;February 28, 2020&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;

&lt;/center&gt;
&lt;p&gt;The tweet is by Benjamin Netanyahu, at the time the prime minister of Israel, who writes:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;â€œIf the recording of Gantzâ€™s advisor is orcherstrated and fabricated (according to Gantzâ€™s words just now), why did Gantz fire him? Gantzâ€™s advisor was fired because he said the truth everyone knows: Gantz canâ€™t be a prime minister. We can. 2 more mandates to the Likkud and we are taking the country out of the plonter, preventing another election and form a governmentâ€&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;This came after the exposure of a secret recording of Gantz in a closed meeting, A few days before election day.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;wordcloud-and-bigrams&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Wordcloud and bigrams&lt;/h2&gt;
&lt;p&gt;Letâ€™s have a look at two more text-related analyses:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;A word-cloud&lt;br /&gt;
&lt;/li&gt;
&lt;li&gt;Bigrams (two-words) from our text&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;We could try out more algorthims but Iâ€™ll save them for a different post (feel free to try on your own).&lt;/p&gt;
&lt;div id=&#34;wordcloud&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Wordcloud&lt;/h3&gt;
&lt;p&gt;In order to tackle the wordcloud, Iâ€™ll break up all the tweets into &lt;strong&gt;single words&lt;/strong&gt;, filter any Hebrew stop words (file found online) and all English words. The decision to filter English words is mainly because Iâ€™m interested in the Hebrew sentences, but also because most the common English words used in our data are those of Twitter user names cited when replying to a tweet:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;he_stopwords &amp;lt;- read_tsv(&amp;quot;https://raw.githubusercontent.com/gidim/HebrewStopWords/master/heb_stopwords.txt&amp;quot;, col_names = &amp;quot;word&amp;quot;)

election_token &amp;lt;- elections %&amp;gt;% 
  unnest_tokens(word, text) %&amp;gt;% 
  select(word) %&amp;gt;%
  anti_join(he_stopwords) %&amp;gt;% 
  count(word, sort = T) %&amp;gt;%
  filter(!grepl(&amp;quot;([a-z]+|×‘×—×™×¨×•×ª)&amp;quot;, word), n&amp;gt;= 150)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we can create a wordcloud of words appearing more than 150 times using &lt;code&gt;{wordcloud2}&lt;/code&gt; package&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;wordcloud2::wordcloud2(election_token, color = &amp;quot;#1DA1F2&amp;quot;, shape = &amp;quot;circle&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: center&#34;&gt;&lt;span id=&#34;fig:unnamed-chunk-13&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;wc.png&#34; alt=&#34;Wordcloud excludes Hebrew stop words and the word &#39;elections&#39;&#34; width=&#34;550&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 1: Wordcloud excludes Hebrew stop words and the word â€˜electionsâ€™
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;What we can see is many of the words weâ€™d expect: Political candidates, government, fourth (in the context of fourth elections), partisâ€™ names and more. Iâ€™ll provide a more thorough discussion following our bigram plot below, as I believe it addresses many of the same words.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;common-bigrams&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Common Bigrams&lt;/h3&gt;
&lt;p&gt;Like we did before, we can break up our text data into &lt;strong&gt;two word&lt;/strong&gt; observations, also known as bigrams. In order to account for all combinations, we break up the sentence to fit all possible options. For example, assume we have the following sentence:&lt;/p&gt;
&lt;p&gt;â€œDanny went to vote yesterdayâ€&lt;/p&gt;
&lt;p&gt;Using the &lt;code&gt;unnest_tokens&lt;/code&gt; weâ€™ll break the sentence into the following bigrams:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Danny went&lt;br /&gt;
&lt;/li&gt;
&lt;li&gt;went to&lt;br /&gt;
&lt;/li&gt;
&lt;li&gt;to vote&lt;br /&gt;
&lt;/li&gt;
&lt;li&gt;vote yesterday&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Which gives us all possible options. We will also include two columns consisting of the bigram broken up into single words. This will help in filtering out bigrams containing Hebrew stop words or English words. Iâ€™ll not run through the following code but instead will point you to &lt;a href=&#34;http://varianceexplained.org/&#34;&gt;David Ronbinson&lt;/a&gt; &amp;amp; &lt;a href=&#34;https://juliasilge.com/&#34;&gt;Julia Silge&lt;/a&gt; &lt;a href=&#34;https://www.tidytextmining.com/&#34;&gt;â€˜Text Mining with Râ€™ Book&lt;/a&gt; for further reading.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;elec_bigram &amp;lt;- elections %&amp;gt;%
  select(text) %&amp;gt;% 
  unnest_tokens(bigram, text, token = &amp;quot;ngrams&amp;quot;, n = 2) %&amp;gt;%
  separate(bigram, into = c(&amp;quot;word1&amp;quot;, &amp;quot;word2&amp;quot;), sep = &amp;quot; &amp;quot;, remove = FALSE) %&amp;gt;% 
  filter(!word1 %in% he_stopwords$word,
         !word2 %in% he_stopwords$word,
         !grepl(&amp;quot;([a-z]+|×‘×—×™×¨×•×ª)&amp;quot;, bigram)) %&amp;gt;% 
  count(word1, word2, sort = T) %&amp;gt;% 
  slice(1:45) %&amp;gt;%
  graph_from_data_frame()

p_arrow &amp;lt;- arrow(type = &amp;quot;closed&amp;quot;, length = unit(.1, &amp;quot;inches&amp;quot;))

ggraph(elec_bigram, layout = &amp;quot;fr&amp;quot;)+
  geom_edge_link(aes(edge_alpha = n), arrow = p_arrow, end_cap = circle(.04, &amp;quot;inches&amp;quot;), show.legend = FALSE)+
  geom_node_point(color = &amp;quot;lightblue&amp;quot;, size = 3)+
  geom_node_text(aes(label = name), vjust = 1, hjust = 1, family = &amp;quot;Calibri&amp;quot;)+
  theme_void()+
  labs(title = &amp;quot;Twitter text bigram&amp;quot;)+
  theme(text = element_text(family = &amp;quot;Calibri&amp;quot;),
        plot.title = element_text(hjust = 0.5 , face = &amp;quot;bold&amp;quot;, size = 18))&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: center&#34;&gt;&lt;span id=&#34;fig:unnamed-chunk-15&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;/post/elections-twitter/index_files/figure-html/unnamed-chunk-15-1.png&#34; alt=&#34;Word bigram excludes Hebrew stop words and the word &#39;elections&#39;&#34; width=&#34;672&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 2: Word bigram excludes Hebrew stop words and the word â€˜electionsâ€™
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;How should we read this graph?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;First off, We only plotted the 45 most common bigrams (out of 100,000+). Every word is connected to another word with an arrow pointing to a given direction. The direction to which the arrow points is the way to read that bigram. In addition, bolder lines represent a higher frequency of that bigram throughout all our text.&lt;br /&gt;
For example, on the bottom of our graph we see the number â€˜2â€™ connected to the words â€˜mandatesâ€™ and â€˜campaginâ€™. The direction of the arrow signals that we should read the bigram as â€˜2 mandatesâ€™ and â€˜2 campaginsâ€™.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;What does this all mean?&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;We have discussions regarding the &lt;strong&gt;number of chairs a govenrment will have (62/61/60/58)&lt;/strong&gt; connected to mentions of the number of election campaigns (2/3) we had, discussions of a united and/or minimal government and the forming of one in general.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;We see &lt;strong&gt;mentions of individuals&lt;/strong&gt; such as â€œBenjamin Netanyahuâ€, â€œAmit Segalâ€ (Both we discussed earlier), â€œNatan Eshelâ€, &lt;strong&gt;but no mention of the main candidate running against Netanyahu - â€œBenny Gantzâ€&lt;/strong&gt;. Thatâ€™s actually kind of odd, but more on that in a minute.&lt;br /&gt;
&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;We also see mentions of political parties such as â€œMeretzâ€, â€œGesherâ€ and â€œLaborâ€ who ran together this time around, â€œOtzma Yehuditâ€, â€œUnited Torah Judaismâ€, and the â€œJoint Listâ€. &lt;strong&gt;Thereâ€™s no mention of the two leading parties - â€œKahol Lavanâ€ &amp;amp; â€œThe Likkudâ€.&lt;/strong&gt;, despite the mentioning of the latterâ€™s leader.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Mentions of Netanyahuâ€™s indicment and the personal law associated him.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Mentions Iâ€™d categorize as â€˜otherâ€™ such as â€œTerrorist supportersâ€, â€œWill of the peopleâ€, â€œFake newsâ€, &#34;Go voteâ€™, etc.
&lt;br&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Actaully, this turned out more interesting than I thought. Several questions arose while looking at it: Several words are missing such as the main parties names (Likkud &amp;amp; Kahol-Lavan), The leading oponent running against Benjamin Netanyahu - Benny Gantz - and other questions such as with whom are specific terms associated. Before we close up Iâ€™ll look at one question that troubles me - &lt;strong&gt;Why doesnâ€™t Gantz appear in our list&lt;/strong&gt; ğŸ˜²?&lt;/p&gt;
&lt;div id=&#34;benny-gantzs-disappearance&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Benny Gantzâ€™s disappearance&lt;/h4&gt;
&lt;p&gt;In order to see why Benny Gantz doesnâ€™t appear in our bigram plot Iâ€™ll do the following: Iâ€™ll break the text into bigrams and filter to &lt;strong&gt;have only the bigrams containing the word Gantz&lt;/strong&gt;. Once we have that we can see why he doesnâ€™t appear in our bigram plot despite appearing in our wordcloud.&lt;br /&gt;
Before I run the analysis and give you the answer think for a moment - What was the process of coming up with the bigram? If I chose only the 50 most frequent bigrams, why would a word that appears many times in our text not appear in our bigram list? Alternatively, did we filter anything along the way? Maybe even give the previous chunk another glance before I answer it.&lt;br /&gt;
&lt;br&gt;
Letâ€™s have a look:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;gantz &amp;lt;-elections %&amp;gt;%
  select(text) %&amp;gt;% 
  unnest_tokens(bigram, text, token = &amp;quot;ngrams&amp;quot;, n = 2) %&amp;gt;%
  separate(bigram, into = c(&amp;quot;word1&amp;quot;, &amp;quot;word2&amp;quot;), sep = &amp;quot; &amp;quot;, remove = FALSE) %&amp;gt;% 
  filter(word1 %in% &amp;quot;×’× ×¥&amp;quot; |
         word2 %in% &amp;quot;×’× ×¥&amp;quot;,
         !grepl(&amp;quot;([a-z]+|×‘×—×™×¨×•×ª)&amp;quot;, bigram))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The code is similar to what we did earlier only this time we left &lt;strong&gt;bigrams that match the word we want&lt;/strong&gt; - bigrams containing the word Gantz. Now that we have our list of bigrams, letâ€™s look at the count of bigrams containing the word ×’× ×¥ (â€˜Gantzâ€™):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;gantz %&amp;gt;% 
  count(bigram, sort = T)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 978 x 2
##    bigram         n
##    &amp;lt;chr&amp;gt;      &amp;lt;int&amp;gt;
##  1 ×©×œ ×’× ×¥       160
##  2 ×‘× ×™ ×’× ×¥      138
##  3 ×’× ×¥ ×œ×        90
##  4 ×¢×œ ×’× ×¥        70
##  5 ××ª ×’× ×¥        69
##  6 ×¢× ×’× ×¥        61
##  7 ×× ×’× ×¥        41
##  8 ×’× ×¥ ×”×™×”       25
##  9 ×’× ×¥ ××•        19
## 10 ×’× ×¥ ×œ×™×‘×¨××Ÿ    19
## # ... with 968 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;AHA!&lt;/strong&gt; Now I see what happened. The first bigram is a stop word and the word Gantz (â€˜Of Gantzâ€™). The second bigram should have been included as it is Gantzâ€™s full name - Benny Gantz, which appears 138 times.&lt;br /&gt;
So, why has it been filtered? This is a great question which we can answer if we look at our stop words we initially used. Letâ€™s see if it has the word ×‘× ×™ (â€˜bennyâ€™ in Hebrew):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;he_stopwords %&amp;gt;% 
  filter(word == &amp;quot;×‘× ×™&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 1 x 1
##   word 
##   &amp;lt;chr&amp;gt;
## 1 ×‘× ×™&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Yes it does. At the time of writing this blog post it leaves me in a dilemma - Should I change the stop words file I used to a different one or maybe create my own? Or should I continue as is? I think leaving it will teach me (and hopefully whoever read this far) a valuable lesson of always checking your stop words. In a different context the specific bigram wouldnâ€™t have got me thinking, but here it didnâ€™t make sense that our leading candidate was filtered, thus my inquire into what happened. In hebrew the word Benny also means â€˜my sonâ€™, which I wouldnâ€™t describe as a stop word but whoever made the dataset I guess did.&lt;/p&gt;
&lt;p&gt;If you wish to give it a try yourself, you can find the data in the form of an &lt;code&gt;.rds&lt;/code&gt; or smaller &lt;code&gt;.csv&lt;/code&gt; (excludes list columns) in my &lt;a href=&#34;https://github.com/AmitLevinson/amitlevinson.com/blob/master/content/post/elections-twitter&#34;&gt;github repository&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Well then, thatâ€™s all for now folks! &lt;strong&gt;And remember, make sure to validate your stop words dataset!&lt;/strong&gt;
&lt;br&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;The function &lt;code&gt;wordcloud2&lt;/code&gt; we wrote wasnâ€™t actually run because it renders an html object which distorts the post. Instead I used the webshot of our rendered html file, read more about that &lt;a href=&#34;https://www.r-graph-gallery.com/196-the-wordcloud2-library.html&#34;&gt;here&lt;/a&gt;.&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;â†©&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>
